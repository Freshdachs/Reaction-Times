---
title: "Posterior Inference on Histogram"
author: "Marcus Brenscheidt"
date: "9/25/2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
For estimating human reaction times, we want to use data from the Platform Human Benchmark. The reaction times however are only available as a Histogram data. We face 2 challenges:
1. Which model do we choose for our data?
2. How do we infer a posterior distribution based on a histogram?

# Getting the data
Data for human reaction times are available as an aggregate statistic in a plot from Human Benchmark. Since the data is at first only visually available, we looked at the html code for the plot and found the original json-Data source. So we load the Histogram Data directly from the Human Benchmark as a json string and transform it into R datatypes.

Furthermore, we provide our own data over github, which is also available in the JSON format. 

This is how we initalize the data:

```{r}
# data from HB histogram
data_backup <- "[[100.0,356],[105.0,224],[110.0,242],[115.0,197],[120.0,124],[125.0,102],[130.0,58],[135.0,42],[140.0,50],[145.0,47],[150.0,132],[155.0,249],[160.0,332],[165.0,487],[170.0,864],[175.0,1297],[180.0,1981],[185.0,3244],[190.0,4383],[195.0,5362],[200.0,6986],[205.0,9344],[210.0,13285],[215.0,15412],[220.0,19350],[225.0,23433],[230.0,25925],[235.0,29517],[240.0,33990],[245.0,36126],[250.0,37764],[255.0,40064],[260.0,40188],[265.0,41213],[270.0,41240],[275.0,39233],[280.0,37815],[285.0,38181],[290.0,34601],[295.0,30424],[300.0,28837],[305.0,26339],[310.0,23172],[315.0,21715],[320.0,19828],[325.0,18128],[330.0,17492],[335.0,15354],[340.0,14415],[345.0,13540],[350.0,12661],[355.0,11716],[360.0,11777],[365.0,10293],[370.0,9459],[375.0,9978],[380.0,8309],[385.0,7715],[390.0,7356],[395.0,7076],[400.0,6733],[405.0,6546],[410.0,5816],[415.0,5564],[420.0,5214],[425.0,4880],[430.0,4914],[435.0,4623],[440.0,4452],[445.0,3777],[450.0,3652],[455.0,3642],[460.0,3851],[465.0,2925],[470.0,2902],[475.0,2888],[480.0,2960],[485.0,2499],[490.0,2515],[495.0,2228],[500.0,840]]"

#load json
library(jsonlite)
data<-fromJSON("http://www.humanbenchmark.com/tests/reactiontime/statistics/data/monthly")
points <- rep(data[,1],data[,2]) # to actual measures

# Raw JSON Object, needs to be converted to a dataframe
raw_data <- fromJSON("https://raw.githubusercontent.com/Freshdachs/Reaction-Times/master/data.json")

# convert a raw JSON Object to a raw dataframe
to_raw_df <- function(df) {
  tmp <- list()
  l<-1
  for (i in 1:length(df$name)) {
    person <- t(df)[,i]
    for (j in 1:length(person$games[,1])){
      game <- person$games[j,]
      for (time in game$times[[1]]){
        tmp[[l]] <- list(name=person$name,gender=person$gender,age=person$age,datetime=game$datetime,type=game$type,time=time)
        l <- l+1
      }
    }
  }
  as.data.frame(do.call(rbind,tmp))
}

# convert a raw JSON Object to a nice dataframe
to_df<-function(data){
  raw <- to_raw_df(data)
  raw$datetime <- lapply(raw$datetime,function(dt)  strptime(dt, "%F %R:%S") )
  raw$time <- unlist(raw$time)
  raw
}

data_core<-to_df(raw_data)
```

# First exploration
We try to look at data and try to fit it with splines. This is an easy but powerful way to fit arbitrary functions.
```{r}
#fit splines
library(MASS)
fitdistr(rep(data[,1],data[,2]),"normal")
fitted <- splinefun(data[,1],data[,2])
fitted
plot(fitted,from=100,to=500)
```

# Stan Model
Next, we want to fit our model with Stan. We look at 3 possible distribution:
  1. weibull
  2. normal
  3. ex-gaussian

We use Stan to generate samples which estimate the parameters for weibull/normal/ex-gaussian.

```{r}
library(rstan)
library(shinystan)
library(retimes)
library(loo)

#Generate iCDF for data
gen_icdf<- function(data){ 
  cmf <- sapply(1:length(data[,1]),function(i)sum(data[1:i,2])/sum(data[,2]))
   function(x)sapply(x,function(y)data[min(which(cmf>=y)),1])
}

rhist <- function(n, data) sapply(runif(n),gen_icdf(data))

prepare<-function(data, n=1000,m=20) list(total=n,y=rhist(n,data),star_size=m)

prepare_pure <- function(data)list(total=length(data),y=data,star_size=m)


primary_cols <- list(green= "#5FF847",turkis= "#35CEC7",blue= "#6A95FB",pink= "#C63AA0",orange= "#FF8C09",fancy_pink= "#FF0961",dark_blue= "#032F5E")
par(bg="black",col.lab="white",fg="white",col.axis="white",col.main="white")

init <- function(data) function(chain_id)list(tau_e = list(uniform=list(lower=0,upper=100)), sigma_e = list(uniform=list(lower=0,upper=500)), mu_e = list(uniform=list(lower=0,upper=500)))


model <- stan_model(file = 'reactiontimes.stan')
fit <- sampling(model, data = prepare(data), iter = 1000, chains = 4, pars=c("alpha","sigma","mu_n", "sigma_n","mu_e","sigma_e","lambda_e","y_star"), init=init(data))

fit <- sampling(model, data = prepare_pure(sample(data_core$time[data_core$type!="Trial"],1000,replace=T)), iter = 1000, chains = 4, pars=c("alpha","sigma","mu_n", "sigma_n","mu_e","sigma_e","lambda_e"), init=init(data))

fit <- sampling(model, data = prepare_pure(data_core$time), iter = 1000, chains = 4, pars=c("alpha","sigma","mu_n", "sigma_n","mu_e","sigma_e","lambda_e"), init=init(data))

posterior<- function(fit)apply(get_posterior_mean(fit),1,mean)

comp<-function(points,posterior, title="Histogram"){
  #plotting
  par(bg="black",col.lab="white",fg="white",col.axis="white",col.main="white",lwd=1.5)
  hist(points, freq=FALSE,breaks = 30, main=title)
  bounds <- par("usr")
  width<-3
  plot(function(x)dweibull(x,posterior["alpha"],posterior["sigma"]),from=bounds[1], to=bounds[2], add=T,col=primary_cols$green,lwd=width)
  plot(function(x)dnorm(x,posterior["mu_n"],posterior["sigma_n"]),from=bounds[1], to=bounds[2], add=T,col=primary_cols$turkis,lwd=width)
  plot(function(x)dexgauss( x,posterior["mu_e"],posterior["sigma_e"],posterior["lambda_e"]^-1),from=bounds[1], to=bounds[2], add=T, col=primary_cols$fancy_pink,lwd=width)
  library(retimes)
  pars<- mexgauss(points)
  plot(function(x)dexgauss( x,pars["mu"],pars["sigma"],pars["tau"]),from=bounds[1], to=bounds[2], add=T, col=primary_cols$orange,lwd=width)
  legend(x = "topright",inset = 0,legend = c("weibull", "normal", "ex-gaussian","med-gaussian"), col=c(primary_cols$green,primary_cols$turkis,primary_cols$fancy_pink,primary_cols$orange), lwd=5, cex=.5, horiz = FALSE)
  print(fit)
}

comp(data_core$time,posterior(fit))

# Now we can do inference:
# p-test: are you worse than 95% of all
worse_than <- function(q,fit) sort(extract(fit)$y_star)[q*length(extract(fit)$y_star)]
# Inference!
worse_than(0.95,fit)
#launch_shinystan(fit)
```


# Sampling Reaction time models with reaction time data from paper
```{r}
RT <- c(474.688, 506.445, 524.081, 530.672, 530.869,566.984, 582.311, 582.940, 603.574, 792.358)


model <- stan_model(file = 'reactiontimes.stan')

fit_paper <- sampling(model, data = prepare_pure(RT), iter = 1000, chains = 4, pars=c("alpha","sigma","mu_n", "sigma_n","mu_e","sigma_e","lambda_e"))

posterior_paper<- apply(get_posterior_mean(fit_paper),1,mean)

comp(RT,posterior(fit_paper))
```